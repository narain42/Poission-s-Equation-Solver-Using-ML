{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "331f346e-d31c-4ac5-af6f-709613989e0b",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e94934-df50-44b0-9f01-5eba00cf6c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt \n",
    "from matplotlib import cm\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, ReduceLROnPlateau, ModelCheckpoint, Callback\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from time import time, gmtime, strftime, localtime\n",
    "import csv\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98d3996-4764-47c2-b69e-723266326b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import pi, exp\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "## run on CPUs...\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
    "\n",
    "# disable eager excution\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1332e416-9ff4-4d65-8924-5c2ba05fc50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define Geometry and Flow parameters\n",
    "## Here we define main parameters for running the program\n",
    "## pinn_grid = True will be the classic Raissi's grid and method with n = 51\n",
    "## with pinn_grid = False, you can choose your own grid, simulation solution\n",
    "## (set to random)\n",
    "## Re is Reynolds number\n",
    "## EPOCH is number of iterations. At least 10K iterations are required for\n",
    "## reasonable solution\n",
    "\n",
    "## Nsamples is sample size, Originally was set to 405. It should not be lower\n",
    "## than this. It can be higher for better solution.\n",
    "\n",
    "## Nbatch, number of batches. Multiple of 32 is a good number.\n",
    "\n",
    "## Restart_sol = False is the intitial option. With this option set to True\n",
    "## you can restart a soution from last stage. This is very important option\n",
    "## as it helps to restart from unexpected power or inernet failue.\n",
    "\n",
    "## x_l, x_u, y_l, y_u are geometric limits of computational domain. You can\n",
    "## set y_l to -1.5 or -4 to have a deep cavity grid.\n",
    "\n",
    "## vdx and vdy are grid difference in x and y directions\n",
    "\n",
    "## When running a notebook, make sure to restart the kernel for least overhead.\n",
    "\n",
    "## checkpoint_path is an important directory. Its name must be changed to \n",
    "## reflect run conditions. Failure to do will overwrite good saved soutions.\n",
    "## You will experience its importance as you run many cases. I have \n",
    "## incorprated some input dialogues for safe starts.\n",
    "\n",
    "## checkpoint_dir is the directory name.\n",
    "\n",
    "## For boundary conditions, look in create_nn function. Make necessary \n",
    "## changes in required bc_top, bc_bottom, bc_left and bc_right statements.\n",
    "\n",
    "## Happy computing. Always Discover something new and correct deficiencies\n",
    "## in the code for the powering future knowledge and skills. Thank you.\n",
    "\n",
    "pinn_grid = False\n",
    "Re = 400.\n",
    "EPOCH = 2500\n",
    "Nsamples = 405\n",
    "Nbatch = 32\n",
    "n = 41\n",
    "Restart_sol = False\n",
    "\n",
    "x_l, x_u, y_l, y_u = 0, 1, -1, 0\n",
    "vdx = (x_u - x_l) / (n - 1) \n",
    "vdy = (y_u - y_l) / (n - 1)\n",
    "\n",
    "resp_op = input('Are you starting a new solution, y for yes, n for no :')\n",
    "if (resp_op == 'y'):\n",
    "   Restart_sol = False\n",
    "   checkpoint_path = input('Prescribe a meaningful path name like Re400_tr1/cp.ckpt etc :')\n",
    "else:\n",
    "   Restart_sol = True \n",
    "   print('Make sure you use correct checkpoint_path')\n",
    "   checkpoint_path = input('Input previously saved path :')\n",
    "\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e816a3ca-1c8c-43b1-a783-2605f6454211",
   "metadata": {},
   "source": [
    "### 1. PINN for 2D N-S equations (lid driven cavity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee50d0f7-7f05-4957-90f2-92d0df02371a",
   "metadata": {},
   "source": [
    "    1. Incompressible Navier–Stokes equation for 2D fluid case: u(x, y), v(x, y), p(x, y)    , x∈[0, 1], y∈[-1, 0]\n",
    "        - Continuity equation : u_x + v_y = 0\n",
    "        - Momentum equation 1 : u*u_x + v*u_y = -p_x + vis*(u_xx + u_yy)\n",
    "        - Momentum equation 2 : u*v_x + v*v_y = -p_y + vis*(v_xx + v_yy)\n",
    "        \n",
    "    2. BC:\n",
    "        - Top    : u = 1 , v = 0 , p_n = 0\n",
    "        - Left   : u = 0 , v = 0 , p_n = 0\n",
    "        - Right  : u = 0 , v = 0 , p_n = 0\n",
    "        - Bottom : u = 0 , v = 0 , p_n = 0\n",
    "         \n",
    "    3. Constants, coefficients:\n",
    "        - density = 1, therefore not appear in momentum equations\n",
    "        - vis: kinematic viscosity (1 / Re)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae7cdf4-aad1-4afc-9ee6-f69c60534684",
   "metadata": {},
   "source": [
    "#### 1.0. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c778f3-6910-4f05-9edd-f9110fe0636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define sampling plan\n",
    "class SamplingPlan_Fix(keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, data=( ), batch_size=( ), batch_per_epoch=1):\n",
    "        # sampling plan: data=(X_pde, y_pde, X_ic, y_ic, X_bc, y_bc), batch_size=(n_pde, n_ic, n_bc)\n",
    "        self.X, self.y, self.X_ic, self.y_ic, self.X_bc, self.y_bc = data\n",
    "        self.n, self.n_ic, self.n_bc = len(self.X), len(self.X_ic), len(self.X_bc)\n",
    "        self.ID, self.ID_ic, self.ID_bc = np.arange(self.n), np.arange(self.n_ic), np.arange(self.n_bc)\n",
    "        # input parameters\n",
    "        self.batch_size, self.batch_ic, self.batch_bc = batch_size\n",
    "        self.batch_per_epoch = batch_per_epoch\n",
    "        \n",
    "    def __len__(self):\n",
    "        # number of mini batch per epoch\n",
    "        return self.batch_per_epoch\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # shuffle & pick collocation sample\n",
    "        np.random.shuffle(self.ID)\n",
    "        idxs = self.ID[:self.batch_size]\n",
    "        batch_X, batch_y = self.X[idxs], self.y[idxs]\n",
    "        # shuffle & pick ic sample\n",
    "        np.random.shuffle(self.ID_ic)\n",
    "        idxs = self.ID_ic[:self.batch_ic]\n",
    "        batch_X_ic, batch_y_ic = self.X_ic[idxs], self.y_ic[idxs]            \n",
    "        # shuffle & pick bc sample \n",
    "        np.random.shuffle(self.ID_bc)\n",
    "        idxs = self.ID_bc[:self.batch_bc]\n",
    "        batch_X_bc, batch_y_bc = self.X_bc[idxs], self.y_bc[idxs]\n",
    "        # combine all sample\n",
    "        batch_X, batch_y = np.vstack([batch_X, batch_X_ic, batch_X_bc]), np.vstack([batch_y, batch_y_ic, batch_y_bc])        \n",
    "        return (batch_X, batch_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e93049f-6922-4278-919a-bf17a63e8678",
   "metadata": {},
   "source": [
    "#### 1.3. a-PINN / n-PINN / can-PINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c4737f-ce01-4d73-9254-7a5e1ab6157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify a-PINN / n-PINN / can-PINN\n",
    "def create_nn(scheme, ff, n_ffs, sigma, lmbda, n_nodes, acf, lr_int):\n",
    "    # input layers -> split into (x, y, dx, dy)\n",
    "    inputs = layers.Input(shape=(4,))\n",
    "    x, y, dx, dy = layers.Lambda( lambda k: tf.split(k, num_or_size_splits=4, axis=1))(inputs)\n",
    "    \n",
    "    # features mapping\n",
    "    initializer_ff = tf.keras.initializers.TruncatedNormal(stddev=sigma)  # features initializer\n",
    "    \n",
    "    if (ff == 'FF'):\n",
    "        hidden_f0 = layers.Dense(n_ffs, activation='linear', use_bias=False, kernel_initializer=initializer_ff)(layers.Concatenate()([x, y]))\n",
    "        hidden_sin, hidden_cos = tf.math.sin(2*tf.constant(pi)*hidden_f0), tf.math.cos(2*tf.constant(pi)*hidden_f0)\n",
    "        hidden_ff = layers.Concatenate()([hidden_sin, hidden_cos])\n",
    "        \n",
    "    if (ff == 'SF') or (ff == 'SIREN'):\n",
    "        hidden_f0 = layers.Dense(n_ffs*2, activation='linear', kernel_initializer=initializer_ff)(layers.Concatenate()([x, y]))\n",
    "        hidden_ff = tf.math.sin(2*tf.constant(pi)*hidden_f0)\n",
    "\n",
    "    if (ff == 'HF'):\n",
    "        hidden_ff = layers.Dense(n_ffs*2, activation=acf, kernel_initializer=initializer_ff)(layers.Concatenate()([x, y]))\n",
    "\n",
    "    # hidden layers\n",
    "    if (ff == 'SIREN'):\n",
    "        initializer = tf.keras.initializers.HeUniform()  # hidden layers initializer\n",
    "        hidden_1 = layers.Dense(n_nodes, activation=acf, kernel_initializer=initializer)(hidden_ff)\n",
    "        hidden_2 = layers.Dense(n_nodes, activation=acf, kernel_initializer=initializer)(hidden_1)\n",
    "        hidden_l = layers.Dense(n_nodes, activation=acf, kernel_initializer=initializer)(hidden_2)\n",
    "    else:\n",
    "        initializer = tf.keras.initializers.GlorotUniform()  # hidden layers initializer\n",
    "        hidden_1 = layers.Dense(n_nodes, activation=acf, kernel_initializer=initializer)(hidden_ff)\n",
    "        hidden_2 = layers.Dense(n_nodes, activation=acf, kernel_initializer=initializer)(hidden_1)\n",
    "        hidden_l = layers.Dense(n_nodes, activation=acf, kernel_initializer=initializer)(hidden_2)\n",
    "\n",
    "    # split layers - u\n",
    "    if (ff == 'SIREN'):\n",
    "        hidden_u1 = layers.Dense(n_nodes, activation=acf, kernel_initializer=initializer)(hidden_l)\n",
    "        hidden_u2 = layers.Dense(n_nodes, activation=acf, kernel_initializer=initializer)(hidden_u1)\n",
    "        hidden_ul = layers.Dense(n_nodes, activation=acf, kernel_initializer=initializer)(hidden_u2)\n",
    "    else:\n",
    "        hidden_u1 = layers.Dense(n_nodes, activation=acf, kernel_initializer=initializer)(hidden_l)\n",
    "        hidden_u2 = layers.Dense(n_nodes, activation=acf, kernel_initializer=initializer)(hidden_u1)\n",
    "        hidden_ul = layers.Dense(n_nodes, activation=acf, kernel_initializer=initializer)(hidden_u2)\n",
    "\n",
    "    # split layers - v\n",
    "    if (ff == 'SIREN'):\n",
    "        hidden_v1 = layers.Dense(n_nodes, activation=acf, kernel_initializer=initializer)(hidden_l)\n",
    "        hidden_v2 = layers.Dense(n_nodes, activation=acf, kernel_initializer=initializer)(hidden_v1)\n",
    "        hidden_vl = layers.Dense(n_nodes, activation=acf, kernel_initializer=initializer)(hidden_v2)\n",
    "    else:\n",
    "        hidden_v1 = layers.Dense(n_nodes, activation=acf, kernel_initializer=initializer)(hidden_l)\n",
    "        hidden_v2 = layers.Dense(n_nodes, activation=acf, kernel_initializer=initializer)(hidden_v1)\n",
    "        hidden_vl = layers.Dense(n_nodes, activation=acf, kernel_initializer=initializer)(hidden_v2)  \n",
    "        \n",
    "    # split layers - p\n",
    "    if (ff == 'SIREN'):\n",
    "        hidden_p1 = layers.Dense(n_nodes, activation=acf, kernel_initializer=initializer)(hidden_l)\n",
    "        hidden_p2 = layers.Dense(n_nodes, activation=acf, kernel_initializer=initializer)(hidden_p1)\n",
    "        hidden_pl = layers.Dense(n_nodes, activation=acf, kernel_initializer=initializer)(hidden_p2)\n",
    "    else:\n",
    "        hidden_p1 = layers.Dense(n_nodes, activation=acf, kernel_initializer=initializer)(hidden_l)\n",
    "        hidden_p2 = layers.Dense(n_nodes, activation=acf, kernel_initializer=initializer)(hidden_p1)\n",
    "        hidden_pl = layers.Dense(n_nodes, activation=acf, kernel_initializer=initializer)(hidden_p2)          \n",
    "        \n",
    "    # output layers\n",
    "    u = layers.Dense(1, use_bias=False, name=\"U\")(hidden_ul)\n",
    "    v = layers.Dense(1, use_bias=False, name=\"V\")(hidden_vl)\n",
    "    p = layers.Dense(1, use_bias=False, name=\"P\")(hidden_pl)  \n",
    "    \n",
    "    # initiate model\n",
    "    outputs = layers.Concatenate()([u, v, p]) \n",
    "    nn = models.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # axillary PDE outputs\n",
    "    u_x, u_y = K.gradients(u, x)[0], K.gradients(u, y)[0]\n",
    "    v_x, v_y = K.gradients(v, x)[0], K.gradients(v, y)[0]\n",
    "    p_x, p_y = K.gradients(p, x)[0], K.gradients(p, y)[0]\n",
    "    u_xx, u_yy = K.gradients(u_x, x)[0], K.gradients(u_y, y)[0]\n",
    "    v_xx, v_yy = K.gradients(v_x, x)[0], K.gradients(v_y, y)[0]    \n",
    "\n",
    "    # initial & boundary conditions:\n",
    "    # Top    : u = 1 , v = 0\n",
    "    # Left   : u = 0 , v = 0\n",
    "    # Right  : u = 0 , v = 0\n",
    "    # Bottom : u = 0 , v = 0\n",
    "    _top, _bottom = tf.equal(y, y_u), tf.equal(y, y_l)\n",
    "    _left, _right = tf.equal(x, x_l), tf.equal(x, x_u)\n",
    "    _bc = tf.logical_or( tf.logical_or(_top, _bottom) , tf.logical_or(_left, _right) )\n",
    "\n",
    "    u_top, v_top = tf.boolean_mask(u, _top), tf.boolean_mask(v, _top)\n",
    "    bc_top = tf.compat.v1.losses.mean_squared_error(labels=tf.ones_like(u_top), predictions=u_top) + \\\n",
    "             tf.compat.v1.losses.mean_squared_error(labels=tf.zeros_like(v_top), predictions=v_top)\n",
    "\n",
    "    u_left, v_left = tf.boolean_mask(u, _left & ~_top), tf.boolean_mask(v, _left & ~_top)\n",
    "    bc_left = tf.compat.v1.losses.mean_squared_error(labels=tf.zeros_like(u_left), predictions=u_left) + \\\n",
    "              tf.compat.v1.losses.mean_squared_error(labels=tf.zeros_like(v_left), predictions=v_left)\n",
    "\n",
    "    u_right, v_right = tf.boolean_mask(u, _right & ~_top), tf.boolean_mask(v, _right & ~_top)\n",
    "    bc_right = tf.compat.v1.losses.mean_squared_error(labels=tf.zeros_like(u_right), predictions=u_right) + \\\n",
    "               tf.compat.v1.losses.mean_squared_error(labels=tf.zeros_like(v_right), predictions=v_right)\n",
    "\n",
    "    u_bottom, v_bottom = tf.boolean_mask(u, _bottom), tf.boolean_mask(v, _bottom)\n",
    "    bc_bottom = tf.compat.v1.losses.mean_squared_error(labels=tf.zeros_like(u_bottom), predictions=u_bottom) + \\\n",
    "                tf.compat.v1.losses.mean_squared_error(labels=tf.zeros_like(v_bottom), predictions=v_bottom)\n",
    "    \n",
    "    bc_mse = bc_top + bc_left + bc_right + bc_bottom\n",
    "    \n",
    "    # PDE (NS equation)\n",
    "    # Continuity equation : u_x + v_y = 0\n",
    "    # Momentum equation 1 : u_t + u*u_x + v*u_y = -(1/rho)*p_x + nu*(u_xx + u_yy)\n",
    "    # Momentum equation 2 : v_t + u*v_x + v*v_y = -(1/rho)*p_y + nu*(v_xx + v_yy)\n",
    "    \n",
    "    # auto-differentian PDE (a-pde)\n",
    "    a_residuals_continuity = u_x + v_y\n",
    "    a_residuals_momentum_1 = u*u_x + v*u_y + p_x - 1.0/Re*(u_xx + u_yy)\n",
    "    a_residuals_momentum_2 = u*v_x + v*v_y + p_y - 1.0/Re*(v_xx + v_yy)\n",
    "    \n",
    "    # exclude BC points \n",
    "    a_residuals_continuity = tf.boolean_mask(a_residuals_continuity, ~_bc)\n",
    "    a_residuals_momentum_1 = tf.boolean_mask(a_residuals_momentum_1, ~_bc)\n",
    "    a_residuals_momentum_2 = tf.boolean_mask(a_residuals_momentum_2, ~_bc)\n",
    "    \n",
    "    a_mse_continuity = tf.compat.v1.losses.mean_squared_error(labels=tf.zeros_like(a_residuals_continuity),\n",
    "                                                              predictions=a_residuals_continuity)\n",
    "    a_mse_momentum_1 = tf.compat.v1.losses.mean_squared_error(labels=tf.zeros_like(a_residuals_momentum_1),\n",
    "                                                              predictions=a_residuals_momentum_1)\n",
    "    a_mse_momentum_2 = tf.compat.v1.losses.mean_squared_error(labels=tf.zeros_like(a_residuals_momentum_2),\n",
    "                                                              predictions=a_residuals_momentum_2)\n",
    "    a_pde_mse = a_mse_continuity + a_mse_momentum_1 + a_mse_momentum_2\n",
    "    a_pde_mse = a_pde_mse / lmbda     \n",
    "    \n",
    "    \n",
    "    # numerical differentiation PDE (n-pde)\n",
    "    # dx & dy get from input\n",
    "    xE, xW = x + dx, x - dx\n",
    "    yN, yS = y + dy, y - dy\n",
    "    uvpE  = nn(tf.stack([xE, y, dx, dy], 1))\n",
    "    uvpW  = nn(tf.stack([xW, y, dx, dy], 1))\n",
    "    uvpN  = nn(tf.stack([x, yN, dx, dy], 1))\n",
    "    uvpS  = nn(tf.stack([x, yS, dx, dy], 1))\n",
    "    uE, vE, pE  = tf.split(uvpE, num_or_size_splits=3, axis=1)\n",
    "    uW, vW, pW  = tf.split(uvpW, num_or_size_splits=3, axis=1)\n",
    "    uN, vN, pN  = tf.split(uvpN, num_or_size_splits=3, axis=1)\n",
    "    uS, vS, pS  = tf.split(uvpS, num_or_size_splits=3, axis=1)\n",
    "    \n",
    "    # second order\n",
    "    xEE, xWW = x + 2.0*dx, x - 2.0*dx\n",
    "    yNN, ySS = y + 2.0*dy, y - 2.0*dy    \n",
    "    uvpEE = nn(tf.stack([xEE, y, dx, dy], 1))\n",
    "    uvpWW = nn(tf.stack([xWW, y, dx, dy], 1))\n",
    "    uvpNN = nn(tf.stack([x, yNN, dx, dy], 1))\n",
    "    uvpSS = nn(tf.stack([x, ySS, dx, dy], 1))  \n",
    "    uEE, vEE, _ = tf.split(uvpEE, num_or_size_splits=3, axis=1)\n",
    "    uWW, vWW, _ = tf.split(uvpWW, num_or_size_splits=3, axis=1)\n",
    "    uNN, vNN, _ = tf.split(uvpNN, num_or_size_splits=3, axis=1)\n",
    "    uSS, vSS, _ = tf.split(uvpSS, num_or_size_splits=3, axis=1)\n",
    "    \n",
    "    uc_e, uc_w = 0.5*(uE + u), 0.5*(uW + u) \n",
    "    vc_n, vc_s = 0.5*(vN + v), 0.5*(vS + v)\n",
    "    div = (uc_e - uc_w) /dx + (vc_n - vc_s) /dy\n",
    "    \n",
    "    # 2nd upwind\n",
    "    Uem_uw2 = 1.5*u  - 0.5*uW\n",
    "    Uep_uw2 = 1.5*uE - 0.5*uEE  \n",
    "    Uwm_uw2 = 1.5*uW - 0.5*uWW\n",
    "    Uwp_uw2 = 1.5*u  - 0.5*uE\n",
    "    Ue_uw2 = tf.where(tf.greater_equal(uc_e, 0.0), Uem_uw2, Uep_uw2)\n",
    "    Uw_uw2 = tf.where(tf.greater_equal(uc_w, 0.0), Uwm_uw2, Uwp_uw2)\n",
    "        \n",
    "    Unm_uw2 = 1.5*u  - 0.5*uS\n",
    "    Unp_uw2 = 1.5*uN - 0.5*uNN    \n",
    "    Usm_uw2 = 1.5*uS - 0.5*uSS\n",
    "    Usp_uw2 = 1.5*u  - 0.5*uN\n",
    "    Un_uw2 = tf.where(tf.greater_equal(vc_n, 0.0), Unm_uw2, Unp_uw2)\n",
    "    Us_uw2 = tf.where(tf.greater_equal(vc_s, 0.0), Usm_uw2, Usp_uw2)\n",
    "\n",
    "    Vem_uw2 = 1.5*v  - 0.5*vW\n",
    "    Vep_uw2 = 1.5*vE - 0.5*vEE\n",
    "    Vwm_uw2 = 1.5*vW - 0.5*vWW\n",
    "    Vwp_uw2 = 1.5*v  - 0.5*vE\n",
    "    Ve_uw2 = tf.where(tf.greater_equal(uc_e, 0.0), Vem_uw2, Vep_uw2)\n",
    "    Vw_uw2 = tf.where(tf.greater_equal(uc_w, 0.0), Vwm_uw2, Vwp_uw2)\n",
    "        \n",
    "    Vnm_uw2 = 1.5*v  - 0.5*vS\n",
    "    Vnp_uw2 = 1.5*vN - 0.5*vNN    \n",
    "    Vsm_uw2 = 1.5*vS - 0.5*vSS\n",
    "    Vsp_uw2 = 1.5*v  - 0.5*vN\n",
    "    Vn_uw2 = tf.where(tf.greater_equal(vc_n, 0.0), Vnm_uw2, Vnp_uw2)\n",
    "    Vs_uw2 = tf.where(tf.greater_equal(vc_s, 0.0), Vsm_uw2, Vsp_uw2)\n",
    "        \n",
    "    UUx_uw2 = (uc_e*Ue_uw2 - uc_w*Uw_uw2) /dx\n",
    "    VUy_uw2 = (vc_n*Un_uw2 - vc_s*Us_uw2) /dy\n",
    "    UVx_uw2 = (uc_e*Ve_uw2 - uc_w*Vw_uw2) /dx\n",
    "    VVy_uw2 = (vc_n*Vn_uw2 - vc_s*Vs_uw2) /dy\n",
    "    \n",
    "    # 2nd central difference    \n",
    "    Uxx_cd2 = (uE - 2.0*u + uW)/ (dx*dx) \n",
    "    Uyy_cd2 = (uN - 2.0*u + uS)/ (dy*dy) \n",
    "    Vxx_cd2 = (vE - 2.0*v + vW)/ (dx*dx) \n",
    "    Vyy_cd2 = (vN - 2.0*v + vS)/ (dy*dy) \n",
    "\n",
    "    pe_cd2 = (p + pE) /2.0 \n",
    "    pw_cd2 = (pW + p) /2.0 \n",
    "    pn_cd2 = (p + pN) /2.0 \n",
    "    ps_cd2 = (pS + p) /2.0 \n",
    "    \n",
    "    Px_cd2 = (pe_cd2 - pw_cd2) /dx\n",
    "    Py_cd2 = (pn_cd2 - ps_cd2) /dy\n",
    "        \n",
    "    n_residuals_continuity = div\n",
    "    n_residuals_momentum_1 = UUx_uw2 + VUy_uw2 - 1.0/Re *(Uxx_cd2 + Uyy_cd2) - u*div + Px_cd2\n",
    "    n_residuals_momentum_2 = UVx_uw2 + VVy_uw2 - 1.0/Re *(Vxx_cd2 + Vyy_cd2) - v*div + Py_cd2   \n",
    "    \n",
    "    # exclude BC points \n",
    "    n_residuals_continuity = tf.boolean_mask(n_residuals_continuity, ~_bc)\n",
    "    n_residuals_momentum_1 = tf.boolean_mask(n_residuals_momentum_1, ~_bc)\n",
    "    n_residuals_momentum_2 = tf.boolean_mask(n_residuals_momentum_2, ~_bc)\n",
    "    \n",
    "    n_mse_continuity = tf.compat.v1.losses.mean_squared_error(labels=tf.zeros_like(n_residuals_continuity),\n",
    "                                                              predictions=n_residuals_continuity)\n",
    "    n_mse_momentum_1 = tf.compat.v1.losses.mean_squared_error(labels=tf.zeros_like(n_residuals_momentum_1),\n",
    "                                                              predictions=n_residuals_momentum_1)\n",
    "    n_mse_momentum_2 = tf.compat.v1.losses.mean_squared_error(labels=tf.zeros_like(n_residuals_momentum_2),\n",
    "                                                              predictions=n_residuals_momentum_2)\n",
    "    n_pde_mse = n_mse_continuity + n_mse_momentum_1 + n_mse_momentum_2\n",
    "    n_pde_mse = n_pde_mse / lmbda     \n",
    "    \n",
    "    \n",
    "    # coupled automatic-numerical differentiation PDE (can-pde)\n",
    "    uE_x, uW_x = K.gradients(uE, xE)[0], K.gradients(uW, xW)[0]\n",
    "    uN_y, uS_y = K.gradients(uN, yN)[0], K.gradients(uS, yS)[0]\n",
    "    \n",
    "    vE_x, vW_x = K.gradients(vE, xE)[0], K.gradients(vW, xW)[0]\n",
    "    vN_y, vS_y = K.gradients(vN, yN)[0], K.gradients(vS, yS)[0]   \n",
    "    \n",
    "    pE_x, pW_x = K.gradients(pE, xE)[0], K.gradients(pW, xW)[0]\n",
    "    pN_y, pS_y = K.gradients(pN, yN)[0], K.gradients(pS, yS)[0]        \n",
    "    \n",
    "    # can 2nd upwind\n",
    "    Uem_cuw2 = u  +  u_x*dx /2.0 #+ (uE_x - u_x)*dx /8.0\n",
    "    Uep_cuw2 = uE - uE_x*dx /2.0 #+ (uE_x - u_x)*dx /8.0  \n",
    "    Uwm_cuw2 = uW + uW_x*dx /2.0 #+ (u_x - uW_x)*dx /8.0\n",
    "    Uwp_cuw2 = u  -  u_x*dx /2.0 #+ (u_x - uW_x)*dx /8.0\n",
    "    Ue_cuw2 = tf.where(tf.greater_equal(uc_e, 0.0), Uem_cuw2, Uep_cuw2)\n",
    "    Uw_cuw2 = tf.where(tf.greater_equal(uc_w, 0.0), Uwm_cuw2, Uwp_cuw2)    \n",
    "    \n",
    "    Unm_cuw2 = u  +  u_y*dy /2.0 #+ (uN_y - u_y)*dy /8.0\n",
    "    Unp_cuw2 = uN - uN_y*dy /2.0 #+ (uN_y - u_y)*dy /8.0 \n",
    "    Usm_cuw2 = uS + uS_y*dy /2.0 #+ (u_y - uS_y)*dy /8.0\n",
    "    Usp_cuw2 = u  -  u_y*dy /2.0 #+ (u_y - uS_y)*dy /8.0\n",
    "    Un_cuw2 = tf.where(tf.greater_equal(vc_n, 0.0), Unm_cuw2, Unp_cuw2)\n",
    "    Us_cuw2 = tf.where(tf.greater_equal(vc_s, 0.0), Usm_cuw2, Usp_cuw2)\n",
    "\n",
    "    Vem_cuw2 = v  +  v_x*dx /2.0 #+ (vE_x - v_x)*dx /8.0\n",
    "    Vep_cuw2 = vE - vE_x*dx /2.0 #+ (vE_x - v_x)*dx /8.0\n",
    "    Vwm_cuw2 = vW + vW_x*dx /2.0 #+ (v_x - vW_x)*dx /8.0\n",
    "    Vwp_cuw2 = v  -  v_x*dx /2.0 #+ (v_x - vW_x)*dx /8.0\n",
    "    Ve_cuw2 = tf.where(tf.greater_equal(uc_e, 0.0), Vem_cuw2, Vep_cuw2)\n",
    "    Vw_cuw2 = tf.where(tf.greater_equal(uc_w, 0.0), Vwm_cuw2, Vwp_cuw2)\n",
    "        \n",
    "    Vnm_cuw2 = v  +  v_y*dy /2.0 #+ (vN_y - v_y)*dy /8.0\n",
    "    Vnp_cuw2 = vN - vN_y*dy /2.0 #+ (vN_y - v_y)*dy /8.0\n",
    "    Vsm_cuw2 = vS + vS_y*dy /2.0 #+ (v_y - vS_y)*dy /8.0\n",
    "    Vsp_cuw2 = v  -  v_y*dy /2.0 #+ (v_y - vS_y)*dy /8.0\n",
    "    Vn_cuw2 = tf.where(tf.greater_equal(vc_n, 0.0), Vnm_cuw2, Vnp_cuw2)\n",
    "    Vs_cuw2 = tf.where(tf.greater_equal(vc_s, 0.0), Vsm_cuw2, Vsp_cuw2)    \n",
    "    \n",
    "    UUx_cuw2 = (uc_e*Ue_cuw2 - uc_w*Uw_cuw2) /dx\n",
    "    VUy_cuw2 = (vc_n*Un_cuw2 - vc_s*Us_cuw2) /dy\n",
    "    UVx_cuw2 = (uc_e*Ve_cuw2 - uc_w*Vw_cuw2) /dx\n",
    "    VVy_cuw2 = (vc_n*Vn_cuw2 - vc_s*Vs_cuw2) /dy       \n",
    "    \n",
    "    # can 2nd central difference    \n",
    "    pe_ccd2 = (p + pE) /2.0 - (pE_x - p_x)*dx /8.0\n",
    "    pw_ccd2 = (pW + p) /2.0 - (p_x - pW_x)*dx /8.0\n",
    "    pn_ccd2 = (p + pN) /2.0 - (pN_y - p_y)*dy /8.0\n",
    "    ps_ccd2 = (pS + p) /2.0 - (p_y - pS_y)*dy /8.0\n",
    "        \n",
    "    Px_ccd2 = (pe_ccd2 - pw_ccd2) /dx\n",
    "    Py_ccd2 = (pn_ccd2 - ps_ccd2) /dy    \n",
    "    \n",
    "    can_residuals_continuity = div\n",
    "    can_residuals_momentum_1 = UUx_cuw2 + VUy_cuw2 - 1.0/Re *(Uxx_cd2 + Uyy_cd2) - u*div + Px_ccd2\n",
    "    can_residuals_momentum_2 = UVx_cuw2 + VVy_cuw2 - 1.0/Re *(Vxx_cd2 + Vyy_cd2) - v*div + Py_ccd2\n",
    "\n",
    "    # exclude BC points \n",
    "    can_residuals_continuity = tf.boolean_mask(can_residuals_continuity, ~_bc)\n",
    "    can_residuals_momentum_1 = tf.boolean_mask(can_residuals_momentum_1, ~_bc)\n",
    "    can_residuals_momentum_2 = tf.boolean_mask(can_residuals_momentum_2, ~_bc)\n",
    "    \n",
    "    can_mse_continuity = tf.compat.v1.losses.mean_squared_error(labels=tf.zeros_like(can_residuals_continuity),\n",
    "                                                                predictions=can_residuals_continuity)\n",
    "    can_mse_momentum_1 = tf.compat.v1.losses.mean_squared_error(labels=tf.zeros_like(can_residuals_momentum_1),\n",
    "                                                                predictions=can_residuals_momentum_1)\n",
    "    can_mse_momentum_2 = tf.compat.v1.losses.mean_squared_error(labels=tf.zeros_like(can_residuals_momentum_2),\n",
    "                                                                predictions=can_residuals_momentum_2)\n",
    "    can_pde_mse = can_mse_continuity + can_mse_momentum_1 + can_mse_momentum_2\n",
    "    can_pde_mse = can_pde_mse / lmbda  \n",
    "    \n",
    "    \n",
    "    # which method to use for PDE loss computation? a-PDE or n-PDE or can-PDE\n",
    "    if (scheme == 'a-pde'):\n",
    "        pde_mse = a_pde_mse\n",
    "    if (scheme == 'n-pde'):\n",
    "        pde_mse = n_pde_mse\n",
    "    if (scheme == 'can-pde'):\n",
    "        pde_mse = can_pde_mse    \n",
    "\n",
    "        \n",
    "    # optimizer\n",
    "    optimizer = tf.keras.optimizers.Adam(lr_int)\n",
    "\n",
    "    # compile model with [?] loss\n",
    "    nn.compile(loss = compute_physics_loss(pde_mse, bc_mse),\n",
    "               optimizer = optimizer,\n",
    "               metrics = [compute_u_loss(dx), compute_v_loss(dy),\n",
    "                          compute_bc_loss(bc_mse), compute_pde_loss(pde_mse)])\n",
    "\n",
    "    # pathway to NN inside variables\n",
    "    insiders = [u, v, p, pde_mse, bc_mse]\n",
    "    eval_ins = K.function([nn.input, K.learning_phase()], insiders)   # evaluation function\n",
    "\n",
    "    return (nn, eval_ins)\n",
    "\n",
    "\n",
    "# loss functions\n",
    "# define loss function (data loss)\n",
    "def compute_data_loss():\n",
    "    def data_loss(y_true, y_pred):\n",
    "        return tf.compat.v1.losses.mean_squared_error(labels=y_true, predictions=y_pred)\n",
    "    return data_loss\n",
    "\n",
    "# define loss function (u loss)\n",
    "def compute_u_loss(dx):\n",
    "    def u_loss(y_true, y_pred):\n",
    "        labels = tf.concat([tf.tile(tf.greater_equal(dx, 0), (1, 1)),\n",
    "                            tf.tile(tf.equal(dx, -1), (1, 2))], 1)\n",
    "        return tf.compat.v1.losses.mean_squared_error(labels=tf.boolean_mask(y_true, labels),\n",
    "                                                      predictions=tf.boolean_mask(y_pred, labels))        \n",
    "    return u_loss # return a function\n",
    "\n",
    "# define loss function (v loss)\n",
    "def compute_v_loss(dx):\n",
    "    def v_loss(y_true, y_pred):\n",
    "        labels = tf.concat([tf.tile(tf.equal(dx, -1), (1, 1)),\n",
    "                            tf.tile(tf.greater_equal(dx, 0), (1, 1)),\n",
    "                            tf.tile(tf.equal(dx, -1), (1, 1))], 1)\n",
    "        return tf.compat.v1.losses.mean_squared_error(labels=tf.boolean_mask(y_true, labels),\n",
    "                                                      predictions=tf.boolean_mask(y_pred, labels))    \n",
    "    return v_loss # return a function\n",
    "\n",
    "# define loss function (p loss)\n",
    "def compute_p_loss(dx):\n",
    "    def p_loss(y_true, y_pred):\n",
    "        labels = tf.concat([tf.tile(tf.equal(dx, -1), (1, 2)),\n",
    "                            tf.tile(tf.greater_equal(dx, 0), (1, 1))], 1)\n",
    "        return tf.compat.v1.losses.mean_squared_error(labels=tf.boolean_mask(y_true, labels),\n",
    "                                                      predictions=tf.boolean_mask(y_pred, labels))    \n",
    "    return p_loss # return a function\n",
    "\n",
    "# define loss function (physics loss)\n",
    "def compute_physics_loss(pde_mse, bc_mse):\n",
    "    def physics_loss(y_true, y_pred): return pde_mse + bc_mse\n",
    "    return physics_loss # return a function\n",
    "\n",
    "# define loss function (BC loss)\n",
    "def compute_bc_loss(bc_mse):\n",
    "    def bc_loss(y_true, y_pred): return bc_mse\n",
    "    return bc_loss # return a function\n",
    "\n",
    "# define loss function (PDE loss)\n",
    "def compute_pde_loss(pde_mse):\n",
    "    def pde_loss(y_true, y_pred): return pde_mse\n",
    "    return pde_loss # return a function\n",
    "\n",
    "\n",
    "\n",
    "# callback: training (prediction & residual) history\n",
    "class TrainingHistory(Callback):\n",
    "    def __init__(self, eval_ins, data):\n",
    "        super(Callback, self).__init__()\n",
    "        self.data = data\n",
    "        self.eval_ins = eval_ins\n",
    "        self.e_hist = []\n",
    "        self.u_hist = []\n",
    "        self.v_hist = []\n",
    "        self.p_hist = []\n",
    "        self.pde_mse_hist = []\n",
    "        self.bc_mse_hist = []\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        e = epoch + 1\n",
    "        if (e < 10) | ((e < 100) & (e%10 == 0)) | ((e < 1000) & (e%100 == 0)) | (e%1000 == 0):\n",
    "            data = self.data\n",
    "            u, v, p, pde_mse, bc_mse = self.eval_ins(data)\n",
    "            self.e_hist.append(e)\n",
    "            self.u_hist.append(u)\n",
    "            self.v_hist.append(v)\n",
    "            self.p_hist.append(p)\n",
    "            self.pde_mse_hist.append(pde_mse)\n",
    "            self.bc_mse_hist.append(bc_mse)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3bb93a-258e-47f7-96d9-fceb29382fd7",
   "metadata": {},
   "source": [
    "### 2. Optimize D-PDE-NN: [SGD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c737d3-41b0-468e-98e7-82ca820a9c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 1.1. Geometry & BC\n",
    "if (pinn_grid == False):\n",
    "    nx = n\n",
    "    ny = n\n",
    "    # computational boundary\n",
    "    ext = [x_l, x_u, y_l, y_u]\n",
    "    x = np.linspace(x_l, x_u, nx)\n",
    "    y = np.linspace(y_l, y_u, ny)\n",
    "\n",
    "    # write a stacked csv file for x and y and read it for X_train\n",
    "    \n",
    "    header = ['x','y']\n",
    "    with open('temp1.csv', 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(header)\n",
    "        for j in range(ny):\n",
    "          for i in range (nx):\n",
    "               data = [(x[i], y[j])]\n",
    "        \n",
    "               writer.writerows(data)\n",
    "    f.close()\n",
    "\n",
    "    #datadir = os.path.join(os.getcwd(), \"../d00_data\")\n",
    "    sim = pd.read_csv( 'temp1.csv')\n",
    "    sim['x'], sim['y'] = sim['x'], sim['y'] - y_u\n",
    "    X_train = np.vstack([sim.x.values, sim.y.values, vdx * np.ones_like(sim.x.values), vdy * np.ones_like(sim.y.values)]).T\n",
    "    y_train = np.random.rand(nx*ny,3)\n",
    "else:\n",
    "    # collect all .csv files from data folder\n",
    "    datadir = os.path.join(os.getcwd(), \"./d00_data\")\n",
    "    sim = pd.read_csv(os.path.join(datadir, 'RE400_LDC_GROUND_TRUTH_51X51.csv'))\n",
    "    sim['x'], sim['y'] = sim['x'], sim['y'] - y_u\n",
    "    X_train = np.vstack([sim.x.values, sim.y.values, vdx * np.ones_like(sim.x.values), vdy * np.ones_like(sim.y.values)]).T\n",
    "    y_train = sim[['u', 'v', 'p']].values\n",
    " \n",
    "   \n",
    "print ('# training sample = %3d  (dx = %.2e, dy = %.2e)' %(len(y_train), vdx, vdy))\n",
    "    \n",
    "\n",
    "# view\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "ax1 = fig.add_subplot(1,2,1); _sc = 1.\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train[:, 0], marker='H', s=10, alpha=.85, cmap='rainbow'); plt.title('u-vel');\n",
    "ax1 = fig.add_subplot(1,2,2); _sc = .5\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train[:, 1], marker='H', s=10, alpha=.85, cmap='rainbow'); plt.title('v-vel');\n",
    "\n",
    "# BC sample\n",
    "\n",
    "bc = np.where((X_train[:, 0] == x_u) | (X_train[:, 0] == x_l) | (X_train[:, 1] == y_u) | (X_train[:, 1] == y_l))[0]\n",
    "X_bc, y_bc = X_train[bc], y_train[bc]\n",
    "    \n",
    "  \n",
    "print ('# BC sample = %d' %len(y_bc))\n",
    "print(X_bc.shape, y_bc.shape)\n",
    "bc_req_len = 4*(n-1)\n",
    "if (len(y_bc) != bc_req_len ):\n",
    "    print('x_bc and y_bc wrong. Should be 4x(n-1)')\n",
    "    exit()\n",
    "\n",
    "# view\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "ax1 = fig.add_subplot(1,2,1); _sc = 1.\n",
    "plt.scatter(X_bc[:, 0], X_bc[:, 1], c=y_bc[:, 0], marker='H', s=10, alpha=.85, cmap='rainbow'); plt.title('u-vel');\n",
    "ax1 = fig.add_subplot(1,2,2); _sc = .5\n",
    "plt.scatter(X_bc[:, 0], X_bc[:, 1], c=y_bc[:, 1], marker='H', s=10, alpha=.85, cmap='rainbow'); plt.title('v-vel');\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58701f44-2a5f-4378-a835-93ef6cc198ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDE\n",
    "BPE = 32\n",
    "LR = 1e-3\n",
    "\n",
    "print(r'Running new Re, $Re=$%d' %Re)\n",
    "\n",
    "# evaluation sample\n",
    "X_eval, y_eval = X_train, y_train\n",
    "\n",
    "# PDE sample\n",
    "X_pde, y_pde = X_train, y_train\n",
    "\n",
    "# IC sample\n",
    "X_ic, y_ic = X_bc, y_bc\n",
    "\n",
    "\n",
    "# initiate NN model (& pathway to internal values)\n",
    "n_ffs = 32\n",
    "lmbda = 1.0\n",
    "\n",
    "ff = 'SIREN'\n",
    "sigma = 1.0\n",
    "\n",
    "# select scheme : 'a-pde', 'n-pde', 'can-pde' \n",
    "scheme = 'can-pde'\n",
    "\n",
    "nn, eval_ins = create_nn(scheme, ff, n_ffs, sigma, lmbda, n_nodes = 20, acf = 'swish', lr_int = 0.001)\n",
    "\n",
    "# first pass\n",
    "u_0, v_0, p_0, pde_mse_0, bc_mse_0 = eval_ins(X_eval) \n",
    "  \n",
    "nn.summary()\n",
    "\n",
    "# PINN training setting: EPOCH, learning rate & sampling strategy\n",
    "\n",
    "DGEN = SamplingPlan_Fix(data=(X_pde, y_pde, X_ic, y_ic, X_bc, y_bc), batch_size=(Nsamples,0,Nbatch), batch_per_epoch=BPE)\n",
    "\n",
    "# callback setting: learning rate schedule & training history\n",
    "lr_sched = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=50, min_lr=5e-6)\n",
    "tr_hist = TrainingHistory(eval_ins, X_eval)\n",
    "# callbacks_list = [lr_sched, tr_hist]\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose = 0)\n",
    "callbacks_list = [lr_sched, tr_hist, cp_callback]\n",
    "    \n",
    "K.set_value(nn.optimizer.lr, LR)  # set learning rate\n",
    "\n",
    "# time it\n",
    "t0 = time()\n",
    "if(Restart_sol == False ):\n",
    "    \n",
    "    # train NN model\n",
    "    history = nn.fit(DGEN, epochs=EPOCH, verbose=2, callbacks=[callbacks_list])\n",
    "  \n",
    "else:\n",
    "    # train from saved chkpoint model\n",
    "    latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    nn.load_weights(latest)\n",
    "    K.set_value(nn.optimizer.lr, LR) \n",
    "    history = nn.fit(DGEN, epochs=EPOCH, verbose=2, callbacks=[callbacks_list])\n",
    "    \n",
    "print (\"...\\nRunning time: %d mins %d secs!\" %(int(time()-t0)/60, np.remainder(int(time()-t0), 60)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7bb3f5-7946-4f28-99bb-9125399096b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "    # final loss\n",
    "    name_checkpt = 'EPOCH = %05d  LOSS = %.2e  (DATA_U = %.2e, DATA_V = %.2e, BC = %.2e, PDE = %.2e)'\\\n",
    "                    %(EPOCH, history.history['loss'][-1], history.history['u_loss'][-1], history.history['v_loss'][-1],\n",
    "                      history.history['bc_loss'][-1], history.history['pde_loss'][-1])\n",
    "    print (name_checkpt)\n",
    "\n",
    "    # plot loss history\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "    fig, axes = plt.subplots(1, figsize=(12, 5))\n",
    "    plt.plot(hist['epoch'], hist['bc_loss'], label=r'BC $residual$', alpha=1, c='orange');\n",
    "    plt.plot(hist['epoch'], hist['pde_loss'], label=r'$%s$-PDEs $residual$'%(scheme.split('-')[0]), alpha=1, c='salmon');\n",
    "    plt.plot(hist['epoch'], hist['loss'], label=r'Training $loss$', alpha=1, c='royalblue'); plt.yscale('log'); plt.grid();\n",
    "    plt.plot(hist['epoch'], hist['u_loss'], label=r'u $loss$', alpha=1, c='springgreen');\n",
    "    plt.plot(hist['epoch'], hist['v_loss'], label=r'v $loss$', alpha=1, c='green');\n",
    "    plt.plot(hist['epoch'], hist['lr'], \"k--\", label=r'Learning rate', alpha=.8, linewidth=1);\n",
    "    plt.xlabel('Epoch', size='x-large'); plt.ylabel('Loss', size='x-large'); plt.xlim((0, EPOCH)); plt.ylim((1e-7, 1e1));\n",
    "    plt.title(r'Training history', fontsize=\"x-large\"); plt.legend(fontsize='x-large', ncol=2);\n",
    "\n",
    "\n",
    "\n",
    "    # new prediction & error on [train] data\n",
    "    p_u, p_v, p_p, _, _ = eval_ins(X_train)\n",
    "\n",
    "    # mse\n",
    "    mse_u =  np.mean((p_u.flatten() - y_train[:, 0].flatten())**2)\n",
    "    mse_v =  np.mean((p_v.flatten() - y_train[:, 1].flatten())**2)\n",
    "    mse_p =  np.mean((p_p.flatten() - y_train[:, 2].flatten())**2)\n",
    "    print ('mse u  = %.3e' %mse_u)\n",
    "    print ('mse v  = %.3e' %mse_v)\n",
    "    print ('mse p  = %.3e' %mse_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035fed84-c52d-4f43-b984-ade69e887f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "   # visualize flow prediction\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "x = np.linspace(x_l, x_u, n)\n",
    "y = np.linspace(y_l, y_u, n)\n",
    "con_lv = 15\n",
    "def accur(ex_fv, pr_fv):\n",
    "  ex_norm = np.linalg.norm(ex_fv)\n",
    "  pr_norm = np.linalg.norm(pr_fv)\n",
    "  accu = 100.*(1. - (abs(ex_norm-pr_norm)/ex_norm))  \n",
    "  return accu\n",
    "_pu, _pv, _pp = p_u.reshape(n, n), p_v.reshape(n, n), p_p.reshape(n, n)\n",
    "_u, _v, _p = y_train[:, 0].reshape(n, n), y_train[:, 1].reshape(n, n), y_train[:, 2].reshape(n, n)\n",
    "\n",
    "psi = np.zeros((n,n))\n",
    "for i in range(n-1):\n",
    "    for j in range(n-1):\n",
    "        psi[i+1,j+1] = psi[i,j+1]+ _pu[i+1, j+1]*(y[j+1]-y[j]) #@- _pv[i+1, j+1]*vdx\n",
    "        \n",
    "# prediction\n",
    "ax1 = fig.add_subplot(2,3,1)\n",
    "plt.contourf(_pu, con_lv, origin='lower', cmap='rainbow', extent=ext, aspect='auto');\n",
    "plt.colorbar(); plt.xlabel('x'); plt.ylabel('y'); plt.title(r'[U] prediction, $Re=$%d' %Re, size='x-large');\n",
    "ax1 = fig.add_subplot(2,3,2)\n",
    "plt.contourf(_pv, con_lv, origin='lower', cmap='rainbow', extent=ext, aspect='auto');\n",
    "plt.colorbar(); plt.xlabel('x'); plt.ylabel('y'); plt.title(r'[V] prediction, $Re=$%d' %Re, size='x-large');\n",
    "ax1 = fig.add_subplot(2,3,3)\n",
    "plt.contourf(_pp, con_lv, origin='lower', cmap='rainbow', extent=ext, aspect='auto');\n",
    "plt.colorbar(); plt.xlabel('x'); plt.ylabel('y'); plt.title(r'[P] prediction, $Re=$%d' %Re, size='x-large');\n",
    "ax1 = fig.add_subplot(2,3,4)\n",
    "plt.contourf(psi, con_lv, origin='lower', cmap='BrBG', extent=ext, aspect='auto');\n",
    "plt.colorbar(); plt.xlabel('x'); plt.ylabel('y'); plt.title(r'psi prediction, $Re=$%d' %Re, size='x-large');\n",
    "# simulation\n",
    "#ax1 = fig.add_subplot(2,3,4)\n",
    "#plt.contourf(_u, con_lv, origin='lower', cmap='rainbow', extent=ext, aspect='auto');\n",
    "#plt.colorbar(); plt.xlabel('x'); plt.ylabel('y'); plt.title(r'[U] simulation, $Re=$%d' %Re, size='x-large');\n",
    "#ax1 = fig.add_subplot(2,3,5)\n",
    "#plt.contourf(_v, con_lv, origin='lower', cmap='rainbow', extent=ext, aspect='auto');\n",
    "#plt.colorbar(); plt.xlabel('x'); plt.ylabel('y'); plt.title(r'[V] simulation, $Re=$%d' %Re, size='x-large');\n",
    "\n",
    "plt.savefig('hdg_results_r1000_1K_second', transparent= True)\n",
    "\n",
    "y_array = [0, 0.0547, 0.0625, 0.0703, 0.1016, 0.1719, 0.2813,\n",
    "                   0.4531, 0.50, 0.6172, 0.7344, 0.8516, 0.9531, \n",
    "                   0.9609, 0.9688, 0.9766, 1.0]\n",
    "    \n",
    "u1000_array = [0.0, -0.1794, -0.2019, -0.2238, -0.3018, -0.3883, -0.2794, -0.1079,\n",
    "                   -0.0620, 0.0580, 0.1908, 0.3383, 0.4776, 0.5284, 0.6009, 0.6756, 1.0]\n",
    "    \n",
    "fig1 = plt.figure(figsize=(12, 5))\n",
    "ax1 = fig1.add_subplot(2, 3, 1)\n",
    "nxp = int((n/2)+1)\n",
    "plt.plot( _pu[:, nxp], x, 'r', label =\"Re=1000\")\n",
    "plt.plot( u1000_array, y_array, 'ro')\n",
    "plt.xlabel('x'); plt.ylabel('y'); plt.title(r'mid section u vel', size='x-large');\n",
    "legend = ax1.legend(loc='best', shadow=True)\n",
    "legend.get_frame().set_facecolor('white')\n",
    "    \n",
    "x_array = [0.0, 0.0625, 0.0703, 0.0781,0.0938,\n",
    "                   0.1563, 0.2266, 0.2344, 0.50, 0.8047,\n",
    "                   0.8594, 0.9063, 0.9453, 0.9531,\n",
    "                   0.9609, 0.9688, 1.0]\n",
    "v1000_array = [0.0, 0.2802, 0.2968, 0.3112, 0.3330, 0.3767, 0.3333, 0.3240, 0.0258,\n",
    "                   -0.3221, -0.4306, -0.5265, -0.3948, -0.3464, -0.2798, -0.2103, 1.0]\n",
    "ax2 = fig1.add_subplot(2, 2,  2)\n",
    "nxp = int((n)/2 + 1)\n",
    "plt.plot( _pv[nxp, :], x, 'm', label =\"Re1000\")\n",
    "plt.plot( v1000_array, x_array, 'm.')\n",
    "plt.xlabel('x'); plt.ylabel('y'); plt.title(r'mid section v vel, $Re=$%d' %Re, size='x-large');\n",
    "legend = ax2.legend(loc='best', shadow=True)\n",
    "legend.get_frame().set_facecolor('white')\n",
    "plt.savefig('hdg_uv_plots_K1_second,Re=%d' %Re, transparent= True)\n",
    "       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cf569d-a9c2-4485-b495-89919bfd8029",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
